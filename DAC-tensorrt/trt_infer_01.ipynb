{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ebd1dd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytools>=2011.2 (from pycuda)\n",
      "  Downloading pytools-2024.1.2-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting appdirs>=1.4.0 (from pycuda)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting mako (from pycuda)\n",
      "  Downloading Mako-1.3.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /home/RRAM_HKU/anaconda3/envs/DAC/lib/python3.9/site-packages (from pytools>=2011.2->pycuda) (4.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /home/RRAM_HKU/anaconda3/envs/DAC/lib/python3.9/site-packages (from pytools>=2011.2->pycuda) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/RRAM_HKU/anaconda3/envs/DAC/lib/python3.9/site-packages (from mako->pycuda) (2.1.5)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading pytools-2024.1.2-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycuda: filename=pycuda-2024.1-cp39-cp39-linux_x86_64.whl size=663060 sha256=f01087f36133ca66ae3f508e294bb7650a2b148d0570840f52e6f25dabecb688\n",
      "  Stored in directory: /home/RRAM_HKU/.cache/pip/wheels/4c/79/a2/654387bb6950d38a2995edd69ef44cbaa1518e9c389cf81e6d\n",
      "Successfully built pycuda\n",
      "Installing collected packages: appdirs, pytools, mako, pycuda\n",
      "Successfully installed appdirs-1.4.4 mako-1.3.3 pycuda-2024.1 pytools-2024.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6256577",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorrt\n",
      "  Downloading tensorrt-10.0.1.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt-cu12 (from tensorrt)\n",
      "  Downloading tensorrt-cu12-10.0.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt-cu12\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.0.1-py2.py3-none-any.whl size=16348 sha256=76799c4e5c3cbcfa240e0ce6470196fcac53962c5ce6db71829be5e6dd7251d7\n",
      "  Stored in directory: /home/RRAM_HKU/.cache/pip/wheels/66/35/d9/b4cc85dc78d44db7878680839a709752259b4b1c90c2a8f309\n",
      "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.0.1-py2.py3-none-any.whl size=17568 sha256=2e0ebc2aa2b4d5032e5030f1025892c651f80c48639f7044559f076bf11f5a30\n",
      "  Stored in directory: /home/RRAM_HKU/.cache/pip/wheels/39/02/07/c74ab0730014857bb0c785fdd43ffd85e6af0a2c25a75b9954\n",
      "Successfully built tensorrt tensorrt-cu12\n",
      "Installing collected packages: tensorrt-cu12, tensorrt\n",
      "Successfully installed tensorrt-10.0.1 tensorrt-cu12-10.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab74b130-5fca-4c36-ad2b-ae5ba3932d43",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import tensorrt as trt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd930d65-1168-472e-ac5c-58b592934211",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set the path to the directory containing the images\n",
    "image_path = \"./JPEGImages\"\n",
    "# model_path = \"./segformer_fp16_2.engine\"\n",
    "model_path = \"./segformer.trt\"\n",
    "# model_path = \"./darknet_fp16.engine\"\n",
    "\n",
    "# Set the input size expected by your TensorRT model\n",
    "input_height = 640\n",
    "input_width = 640\n",
    "input_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88553c60-4558-4cc7-a21c-3357afa98bb7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/04/2024-12:58:22] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[05/04/2024-12:58:22] [TRT] [I] Loaded engine size: 25 MiB\n",
      "[05/04/2024-12:58:22] [TRT] [E] 1: [runtime.cpp::parsePlan::455] Error Code 1: Serialization (Serialization assertion plan->header.magicTag == rt::kPLAN_MAGIC_TAG failed.Trying to load an engine created with incompatible serialization version. Check that the engine was not created using safety runtime, same OS was used and version compatibility parameters were set accordingly.)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create_execution_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     engine \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mdeserialize_cuda_engine(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create execution context\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_execution_context\u001b[49m()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Get input and output names\u001b[39;00m\n\u001b[1;32m     10\u001b[0m input_name \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mget_binding_name(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create_execution_context'"
     ]
    }
   ],
   "source": [
    "# Load the TensorRT model\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "with open(model_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "# Create execution context\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# Get input and output names\n",
    "input_name = engine.get_binding_name(0)\n",
    "output_name = engine.get_binding_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5faedd87-6e2c-4e09-a379-66b248fe99e2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_binding_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Allocate device memory for input and output\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, input_channels, input_height, input_width)\n\u001b[0;32m----> 3\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_binding_shape\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m input_size \u001b[38;5;241m=\u001b[39m trt\u001b[38;5;241m.\u001b[39mvolume(input_shape) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mitemsize\n\u001b[1;32m      5\u001b[0m output_size \u001b[38;5;241m=\u001b[39m trt\u001b[38;5;241m.\u001b[39mvolume(output_shape) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mitemsize\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_binding_shape'"
     ]
    }
   ],
   "source": [
    "# Allocate device memory for input and output\n",
    "input_shape = (1, input_channels, input_height, input_width)\n",
    "output_shape = engine.get_binding_shape(1)\n",
    "input_size = trt.volume(input_shape) * np.dtype(np.float32).itemsize\n",
    "output_size = trt.volume(output_shape) * np.dtype(np.float32).itemsize\n",
    "d_input = cuda.mem_alloc(input_size)\n",
    "d_output = cuda.mem_alloc(output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ff7431-ba96-4330-803a-02ccb4440de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a stream to run inference\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33f4262-e15f-40dc-9e09-8a0c865fb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image files in the directory\n",
    "image_files = [f for f in os.listdir(image_path) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "num_images = len(image_files)\n",
    "\n",
    "def preprocess_image(image_path, input_height, input_width):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to the desired input shape\n",
    "    resized_image = cv2.resize(image, (input_width, input_height))\n",
    "    \n",
    "    # Convert the image from BGR to RGB color space\n",
    "    rgb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Normalize the pixel values to the range [0, 1]\n",
    "    normalized_image = rgb_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Transpose the image to match the input shape (channels first)\n",
    "    transposed_image = normalized_image.transpose((2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension to the image\n",
    "    batch_image = np.expand_dims(transposed_image, axis=0)\n",
    "    \n",
    "    return batch_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ceae380-4703-491e-b8ef-4371d98b6798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/03/2024-23:12:27] [TRT] [E] 1: [genericReformat.cu::executeMemcpy::1455] Error Code 1: Cuda Runtime (invalid argument)\n"
     ]
    },
    {
     "ename": "LogicError",
     "evalue": "cuMemcpyDtoHAsync failed: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Transfer output data from device\u001b[39;00m\n\u001b[1;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(output_shape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemcpy_dtoh_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Synchronize the stream to ensure the inference is complete\u001b[39;00m\n\u001b[1;32m     21\u001b[0m stream\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[0;31mLogicError\u001b[0m: cuMemcpyDtoHAsync failed: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "# Perform inference on each image\n",
    "\n",
    "total_time = 0\n",
    "for image_file in image_files:\n",
    "    # Preprocess the image\n",
    "    image = preprocess_image(os.path.join(image_path, image_file), input_height, input_width)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, image.ravel(), stream)\n",
    "    \n",
    "    # Run inference\n",
    "    context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)\n",
    "    \n",
    "    # Transfer output data from device\n",
    "    output = np.empty(output_shape, dtype=np.float32)\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    print(output)\n",
    "    print(d_output)\n",
    "    \n",
    "    # Synchronize the stream to ensure the inference is complete\n",
    "    stream.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = total_time + (end_time - start_time)\n",
    "    \n",
    "    # Postprocess the output (if needed)\n",
    "    # ...\n",
    "\n",
    "# end_time = time.time()\n",
    "# total_time = end_time - start_time\n",
    "inference_speed = num_images / total_time\n",
    "\n",
    "print(f\"Processed {num_images} images in {total_time:.2f} seconds\")\n",
    "print(f\"Inference speed: {inference_speed:.2f} images per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a02eb-abf8-46c6-a781-e45f3b357c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
