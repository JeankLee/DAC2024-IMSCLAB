{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model_path = \"./yolon_no_prune.onnx\"\n",
    "# session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider']))\n",
    "# session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider']))\n",
    "session = ort.InferenceSession(onnx_model_path, providers=['TensorrtExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "image_folder_path = \"./JPEGImages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    # 左上右下\n",
    "    lt, rb  = torch.split(distance, 2, dim)\n",
    "    x1y1    = anchor_points - lt\n",
    "    x2y2    = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy    = (x1y1 + x2y2) / 2\n",
    "        wh      = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "\n",
    "def decode_box(num_classes, input_shape, dbox, cls, anchors, strides):\n",
    "    # dbox, cls, origin_cls, anchors, strides = inputs\n",
    "    dbox = dist2bbox(dbox, anchors.unsqueeze(0), xywh=True, dim=1) * strides\n",
    "    y = torch.cat((dbox, cls.sigmoid()), 1).permute(0, 2, 1)\n",
    "    y[:, :, :4] = y[:, :, :4] / torch.Tensor([input_shape[1], input_shape[0], input_shape[1], input_shape[0]]).to(y.device)\n",
    "    return y\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image):\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = np.array(input_shape)\n",
    "    image_shape = np.array(image_shape)\n",
    "\n",
    "    if letterbox_image:\n",
    "        new_shape = np.round(image_shape * np.min(input_shape/image_shape))\n",
    "        offset = (input_shape - new_shape)/2./input_shape\n",
    "        scale = input_shape/new_shape\n",
    "\n",
    "        box_yx = (box_yx - offset) * scale\n",
    "        box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1)\n",
    "    boxes *= np.concatenate([image_shape, image_shape], axis=-1)\n",
    "    return boxes\n",
    "\n",
    "def non_max_suppression(prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4):\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for i, image_pred in enumerate(prediction):\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 4:4 + num_classes], 1, keepdim=True)\n",
    "        conf_mask = (class_conf[:, 0] >= conf_thres).squeeze()\n",
    "        image_pred = image_pred[conf_mask]\n",
    "        class_conf = class_conf[conf_mask]\n",
    "        class_pred = class_pred[conf_mask]\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        detections = torch.cat((image_pred[:, :4], class_conf.float(), class_pred.float()), 1)\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "\n",
    "        if prediction.is_cuda:\n",
    "            unique_labels = unique_labels.cuda()\n",
    "            detections = detections.cuda()\n",
    "\n",
    "        for c in unique_labels:\n",
    "            detections_class = detections[detections[:, -1] == c]\n",
    "            keep = nms(detections_class[:, :4], detections_class[:, 4], nms_thres)\n",
    "            max_detections = detections_class[keep]\n",
    "            output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections))\n",
    "        \n",
    "        if output[i] is not None:\n",
    "            output[i] = output[i].cpu().numpy()\n",
    "            box_xy, box_wh = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2]\n",
    "            output[i][:, :4] = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [1280, 1280]\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./JPEGImages/0.jpg', './JPEGImages/1.jpg', './JPEGImages/2.jpg', './JPEGImages/3.jpg', './JPEGImages/4.jpg', './JPEGImages/5.jpg', './JPEGImages/6.jpg', './JPEGImages/7.jpg', './JPEGImages/8.jpg', './JPEGImages/9.jpg', './JPEGImages/10.jpg', './JPEGImages/11.jpg', './JPEGImages/12.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   8%|▊         | 1/13 [00:00<00:05,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/0.jpg\n",
      "image_shape: [ 720 1280]\n",
      "top_label: [0]\n",
      "top_conf: [0.7588528]\n",
      "top_boxes: [[330.1515  463.5061  346.01993 482.37976]]\n",
      "b'0 0.76' 330 463 346 482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  15%|█▌        | 2/13 [00:00<00:04,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/1.jpg\n",
      "image_shape: [ 720 1280]\n",
      "top_label: [0]\n",
      "top_conf: [0.77763134]\n",
      "top_boxes: [[403.4912  332.62305 442.713   375.2893 ]]\n",
      "b'0 0.78' 403 332 442 375\n",
      "./JPEGImages/2.jpg\n",
      "image_shape: [ 720 1280]\n",
      "top_label: [0 0 0 0]\n",
      "top_conf: [0.90948296 0.89358085 0.8629413  0.7074271 ]\n",
      "top_boxes: [[327.31598  -1.47192 484.00757 226.59546]\n",
      " [362.62933 456.36084 459.98343 581.25635]\n",
      " [342.83203 245.34608 393.43063 304.55188]\n",
      " [334.53024 195.3917  370.79248 240.9744 ]]\n",
      "b'0 0.91' 327 0 484 226\n",
      "b'0 0.89' 362 456 459 581\n",
      "b'0 0.86' 342 245 393 304\n",
      "b'0 0.71' 334 195 370 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  23%|██▎       | 3/13 [00:01<00:05,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/3.jpg\n",
      "image_shape: [1080 1920]\n",
      "top_label: [0 0 0 0 0 0 1]\n",
      "top_conf: [0.86952496 0.86128706 0.79200983 0.7785965  0.7571234  0.6416642\n",
      " 0.7644294 ]\n",
      "top_boxes: [[ 513.69556 1419.8953   613.6967  1667.4802 ]\n",
      " [ 521.3985   859.8302   615.79767  985.80334]\n",
      " [ 525.99896  388.0211   588.63226  474.7704 ]\n",
      " [ 529.89044  784.23486  562.3664   823.19   ]\n",
      " [ 531.3195   737.2389   556.40454  764.218  ]\n",
      " [ 525.63354  535.7323   554.7002   579.95245]\n",
      " [ 524.3351  1151.9467   606.939   1193.6678 ]]\n",
      "b'0 0.87' 513 1419 613 1667\n",
      "b'0 0.86' 521 859 615 985\n",
      "b'0 0.79' 525 388 588 474\n",
      "b'0 0.78' 529 784 562 823\n",
      "b'0 0.76' 531 737 556 764\n",
      "b'0 0.64' 525 535 554 579\n",
      "b'1 0.76' 524 1151 606 1193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  38%|███▊      | 5/13 [00:02<00:04,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/4.jpg\n",
      "image_shape: [ 720 1280]\n",
      "top_label: [0]\n",
      "top_conf: [0.74688995]\n",
      "top_boxes: [[380.5364  580.53345 395.8494  598.0051 ]]\n",
      "b'0 0.75' 380 580 395 598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  46%|████▌     | 6/13 [00:03<00:03,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/5.jpg\n",
      "image_shape: [ 720 1280]\n",
      "top_label: [0]\n",
      "top_conf: [0.5746191]\n",
      "top_boxes: [[445.75278 446.6018  463.13177 467.0672 ]]\n",
      "b'0 0.57' 445 446 463 467\n",
      "./JPEGImages/6.jpg\n",
      "image_shape: [1088 1920]\n",
      "top_label: [0 0 1 2]\n",
      "top_conf: [0.8759316  0.86156726 0.8493998  0.67359865]\n",
      "top_boxes: [[ 556.484    710.46185  681.69415  850.79114]\n",
      " [ 577.0404   545.7759   684.98285  680.75415]\n",
      " [ 552.46356  906.0999   648.14197  986.18756]\n",
      " [ 551.38403 1079.0781   631.5888  1105.9963 ]]\n",
      "b'0 0.88' 556 710 681 850\n",
      "b'0 0.86' 577 545 684 680\n",
      "b'1 0.85' 552 906 648 986\n",
      "b'2 0.67' 551 1079 631 1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  54%|█████▍    | 7/13 [00:03<00:03,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/7.jpg\n",
      "image_shape: [1080 1920]\n",
      "top_label: [0 0 0 0 0 0 0 0 0 0 4 5]\n",
      "top_conf: [0.9145428  0.8965528  0.8744245  0.8733102  0.8489684  0.84777987\n",
      " 0.8039182  0.77353615 0.6918869  0.57517165 0.8654269  0.89768445]\n",
      "top_boxes: [[4.78953674e+02 2.12649246e+02 6.29408203e+02 4.82037506e+02]\n",
      " [5.62563721e+02 1.24140755e+02 6.73785645e+02 3.43844452e+02]\n",
      " [5.32186951e+02 9.15501526e+02 6.57321777e+02 1.04398474e+03]\n",
      " [5.53872070e+02 6.66079346e+02 6.85565186e+02 8.33956360e+02]\n",
      " [5.42225647e+02 3.85875031e+02 6.37761353e+02 5.21197754e+02]\n",
      " [5.37519409e+02 7.23595619e-01 6.54739624e+02 1.16539795e+02]\n",
      " [5.56659607e+02 7.98532471e+02 6.18690735e+02 8.63140320e+02]\n",
      " [5.55472290e+02 7.16643219e+01 6.57645813e+02 2.11347153e+02]\n",
      " [5.51677307e+02 5.08522156e+02 6.20945679e+02 5.54957825e+02]\n",
      " [5.58800354e+02 1.03104968e+03 6.15208313e+02 1.05857983e+03]\n",
      " [3.84687195e+02 9.25074524e+02 4.34500458e+02 9.42665344e+02]\n",
      " [3.82269928e+02 1.02110663e+03 4.34565186e+02 1.04556995e+03]]\n",
      "b'0 0.91' 478 212 629 482\n",
      "b'0 0.90' 562 124 673 343\n",
      "b'0 0.87' 532 915 657 1043\n",
      "b'0 0.87' 553 666 685 833\n",
      "b'0 0.85' 542 385 637 521\n",
      "b'0 0.85' 537 0 654 116\n",
      "b'0 0.80' 556 798 618 863\n",
      "b'0 0.77' 555 71 657 211\n",
      "b'0 0.69' 551 508 620 554\n",
      "b'0 0.58' 558 1031 615 1058\n",
      "b'4 0.87' 384 925 434 942\n",
      "b'5 0.90' 382 1021 434 1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  62%|██████▏   | 8/13 [00:04<00:03,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/8.jpg\n",
      "image_shape: [1080 1920]\n",
      "top_label: [0 0 0 0]\n",
      "top_conf: [0.9120898  0.8793356  0.87576884 0.68811953]\n",
      "top_boxes: [[488.19662     1.9689274 758.20984   328.40012  ]\n",
      " [506.0826    555.47406   622.6379    691.66766  ]\n",
      " [460.23618   752.111     606.8192    917.6165   ]\n",
      " [524.45746   723.18994   550.1065    749.72485  ]]\n",
      "b'0 0.91' 488 1 758 328\n",
      "b'0 0.88' 506 555 622 691\n",
      "b'0 0.88' 460 752 606 917\n",
      "b'0 0.69' 524 723 550 749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  69%|██████▉   | 9/13 [00:05<00:02,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/9.jpg\n",
      "image_shape: [1080 1920]\n",
      "top_label: [0 0 0]\n",
      "top_conf: [0.87558335 0.85386    0.7444073 ]\n",
      "top_boxes: [[ 334.07336   1408.817      460.43317   1544.2117   ]\n",
      " [ 337.09003    966.3564     410.7061    1053.1396   ]\n",
      " [ 334.12927      1.6483784  472.5522     141.2091   ]]\n",
      "b'0 0.88' 334 1408 460 1544\n",
      "b'0 0.85' 337 966 410 1053\n",
      "b'0 0.74' 334 1 472 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  77%|███████▋  | 10/13 [00:06<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/10.jpg\n",
      "image_shape: [1080 1920]\n",
      "top_label: [0 6]\n",
      "top_conf: [0.87161624 0.5254349 ]\n",
      "top_boxes: [[ 529.59973  981.4095   607.31726 1072.2589 ]\n",
      " [ 236.51384  181.82945  278.41318  244.2766 ]]\n",
      "b'0 0.87' 529 981 607 1072\n",
      "b'6 0.53' 236 181 278 244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  85%|████████▍ | 11/13 [00:06<00:01,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/11.jpg\n",
      "image_shape: [ 720 1280]\n",
      "top_label: [0 2]\n",
      "top_conf: [0.7758542  0.68949246]\n",
      "top_boxes: [[307.88986 343.11658 339.5513  423.54495]\n",
      " [313.40268 789.3176  370.82462 809.5403 ]]\n",
      "b'0 0.78' 307 343 339 423\n",
      "b'2 0.69' 313 789 370 809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  92%|█████████▏| 12/13 [00:07<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./JPEGImages/12.jpg\n",
      "image_shape: [1080 1920]\n",
      "top_label: [0 0 0 0 0 1]\n",
      "top_conf: [0.89559865 0.8579432  0.84982526 0.6104842  0.53023744 0.5017248 ]\n",
      "top_boxes: [[589.4994    62.665413 751.87225  338.50867 ]\n",
      " [711.85406  657.76013  799.4238   758.24274 ]\n",
      " [744.72864  854.0421   817.8033   912.14264 ]\n",
      " [748.04926  534.5122   778.7087   598.3492  ]\n",
      " [756.8212   617.64185  782.9046   666.5429  ]\n",
      " [774.58887  948.55145  818.86426  976.40814 ]]\n",
      "b'0 0.90' 589 62 751 338\n",
      "b'0 0.86' 711 657 799 758\n",
      "b'0 0.85' 744 854 817 912\n",
      "b'0 0.61' 748 534 778 598\n",
      "b'0 0.53' 756 617 782 666\n",
      "b'1 0.50' 774 948 818 976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 13 images in 7.67 seconds.\n",
      "Frame per second (FPS): 1.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import colorsys\n",
    "\n",
    "# Assuming you have already loaded the model and created a session\n",
    "# session = ort.InferenceSession(onnx_model_path)\n",
    "def preprocess_input(image):\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def resize_image(image, size, letterbox_image):\n",
    "    iw, ih  = image.size\n",
    "    w, h    = size\n",
    "    if letterbox_image:\n",
    "        scale   = min(w/iw, h/ih)\n",
    "        nw      = int(iw*scale)\n",
    "        nh      = int(ih*scale)\n",
    "\n",
    "        image   = image.resize((nw,nh), Image.BICUBIC)\n",
    "        new_image = Image.new('RGB', size, (128,128,128))\n",
    "        new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "    else:\n",
    "        new_image = image.resize((w, h), Image.BICUBIC)\n",
    "    return new_image\n",
    "\n",
    "# Function to preprocess the image (modify as per your model's requirement)\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img  = resize_image(img, (input_shape[1], input_shape[0]), True)\n",
    "    image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(img, dtype='float32')), (2, 0, 1)), 0)\n",
    "    # img = cv2.resize(img, (640, 640))  # Resize to match model requirement\n",
    "    # img = img.transpose(2, 0, 1)  # Change data layout from HWC to CHW\n",
    "    # img = img.astype('float32') / 255.0  # Normalize if required\n",
    "    # img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return image_data\n",
    "import re\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(s) if s.isdigit() else s.lower() for s in re.split('(\\d+)', s)]\n",
    "\n",
    "image_paths = sorted([os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))], key=natural_sort_key)\n",
    "print(image_paths)\n",
    "image_paths = image_paths[:1000]\n",
    "\n",
    "total_time = 0\n",
    "k = 0\n",
    "# Process each image and perform inference with a progress bar\n",
    "for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "    input_image = preprocess_image(image_path)\n",
    "    image = Image.open(image_path)\n",
    "    # Assuming the model takes an input named 'input' and outputs a tensor named 'output'\n",
    "    image_shape = np.array(np.shape(image)[0:2])\n",
    "    start_time = time.time()\n",
    "    outputs = session.run(None, {'input': input_image})\n",
    "    output = [torch.tensor(arr) for arr in outputs]\n",
    "    outputs = [torch.tensor(arr) for arr in outputs]\n",
    "    #0是对的，1是对的，6对4，5对3\n",
    "    outputs = decode_box(num_classes, input_shape, outputs[0], outputs[1], outputs[5], outputs[6])\n",
    "\n",
    "    #######################################################################我加的\n",
    "    hsv_tuples = [(x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "    target_width, target_height = image_shape[1], image_shape[0]\n",
    "    target_aspect = target_width / target_height\n",
    "    new_height = int(input_shape[0] / target_aspect)\n",
    "    offset = (input_shape[1] - new_height) // 2\n",
    "    masks = torch.argmax(output[7].long(), 1)\n",
    "    masks = masks[0, :, :].cpu().detach().numpy()\n",
    "    mask_rgb = np.zeros((*masks.shape, 3), dtype=np.uint8)\n",
    "    mask_rgb[masks == 1] = [255, 0, 0]  # 红色\n",
    "    mask_rgb[masks == 2] = [0, 255, 0]  # 绿色 \n",
    "    mask_rgb[masks == 3] = [0, 0, 255]  # 蓝色\n",
    "    cropped_image = mask_rgb[offset:(offset + new_height), :]\n",
    "    mask_rgb = cv2.resize(cropped_image, (image_shape[1], image_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    mask_image = Image.fromarray(mask_rgb)\n",
    "    #######################################################################我加的\n",
    "\n",
    "    results = non_max_suppression(outputs, num_classes, input_shape, \n",
    "                image_shape, True, conf_thres = 0.5, nms_thres = 0.3)\n",
    "    if results[0] is None: continue\n",
    "    else:\n",
    "        top_label   = np.array(results[0][:, 5], dtype = 'int32')\n",
    "        top_conf    = results[0][:, 4]\n",
    "        top_boxes   = results[0][:, :4]\n",
    "        print(image_path)\n",
    "        print(f'image_shape: {image_shape}')\n",
    "        print(f'top_label: {top_label}')\n",
    "        print(f'top_conf: {top_conf}')\n",
    "        print(f'top_boxes: {top_boxes}')\n",
    "\n",
    "        #---------------------------------------------------------#\n",
    "        #   设置字体与边框厚度\n",
    "        #---------------------------------------------------------#\n",
    "        font        = ImageFont.truetype(font='./simhei.ttf', size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        thickness   = int(max((image.size[0] + image.size[1]) // np.mean(input_shape), 1))\n",
    "        for i, c in list(enumerate(top_label)):\n",
    "            predicted_class = int(c)\n",
    "            box             = top_boxes[i]\n",
    "            score           = top_conf[i]\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "\n",
    "            top     = max(0, np.floor(top).astype('int32'))\n",
    "            left    = max(0, np.floor(left).astype('int32'))\n",
    "            bottom  = min(image.size[1], np.floor(bottom).astype('int32'))\n",
    "            right   = min(image.size[0], np.floor(right).astype('int32'))\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            label_size = draw.textsize(label, font)\n",
    "            label = label.encode('utf-8')\n",
    "            print(label, top, left, bottom, right)\n",
    "            \n",
    "            if top - label_size[1] >= 0:\n",
    "                text_origin = np.array([left, top - label_size[1]])\n",
    "            else:\n",
    "                text_origin = np.array([left, top + 1])\n",
    "\n",
    "            for i in range(thickness):\n",
    "                draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
    "            draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=colors[c])\n",
    "            draw.text(text_origin, str(label,'UTF-8'), fill=(0, 0, 0), font=font)\n",
    "            del draw\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            mask_image = mask_image.convert(\"RGBA\")\n",
    "            mask_array = np.array(mask_image)\n",
    "            mask_array[:, :, 3] = (mask_array[:, :, :3] > 0).any(axis=2) * 128\n",
    "            mask_image = Image.fromarray(mask_array)\n",
    "            image = image.convert(\"RGBA\")\n",
    "            image.alpha_composite(mask_image)\n",
    "            del draw\n",
    "        image.save(f'./output/{k}.png')\n",
    "        k = k + 1\n",
    "                #######################################################################我加的\n",
    "                # draw = ImageDraw.Draw(image)\n",
    "                # mask_image = mask_image.convert(\"RGBA\")\n",
    "                # mask_array = np.array(mask_image)\n",
    "                # mask_array[:, :, 3] = (mask_array[:, :, :3] > 0).any(axis=2) * 128\n",
    "                # mask_image = Image.fromarray(mask_array)\n",
    "                # image = image.convert(\"RGBA\")\n",
    "                # image.alpha_composite(mask_image)\n",
    "                # del draw\n",
    "                #######################################################################我加的\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = total_time + (end_time - start_time)\n",
    "\n",
    "\n",
    "fps = len(image_paths) / total_time\n",
    "\n",
    "print(f\"Processed {len(image_paths)} images in {total_time:.2f} seconds.\")\n",
    "print(f\"Frame per second (FPS): {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
