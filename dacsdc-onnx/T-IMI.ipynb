{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Create Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../common\"))\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# import pynq\n",
    "import dac_sdc\n",
    "from IPython.display import display\n",
    "\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "team_name = 'T-IMI'\n",
    "dac_sdc.BATCH_SIZE = 1\n",
    "team = dac_sdc.Team(team_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the library and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some library\n",
    "# !pip install onnxruntime\n",
    "# !conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"./yolon_no_prune.onnx\"\n",
    "# session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "# session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider']))\n",
    "session = ort.InferenceSession(onnx_model_path, providers=['TensorrtExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Callback Function and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [1280, 1280]\n",
    "num_classes = 7\n",
    "\n",
    "def preprocess_input(image):\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def resize_image(image, size, letterbox_image):\n",
    "    iw, ih  = image.size\n",
    "    w, h    = size\n",
    "    if letterbox_image:\n",
    "        scale   = min(w/iw, h/ih)\n",
    "        nw      = int(iw*scale)\n",
    "        nh      = int(ih*scale)\n",
    "\n",
    "        image   = image.resize((nw,nh), Image.BICUBIC)\n",
    "        new_image = Image.new('RGB', size, (128,128,128))\n",
    "        new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "    else:\n",
    "        new_image = image.resize((w, h), Image.BICUBIC)\n",
    "    return new_image\n",
    "\n",
    "# Function to preprocess the image (modify as per your model's requirement)\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img  = resize_image(img, (input_shape[1], input_shape[0]), True)\n",
    "    image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(img, dtype='float32')), (2, 0, 1)), 0)\n",
    "    return image_data\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    # 左上右下\n",
    "    lt, rb  = torch.split(distance, 2, dim)\n",
    "    x1y1    = anchor_points - lt\n",
    "    x2y2    = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy    = (x1y1 + x2y2) / 2\n",
    "        wh      = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "\n",
    "def decode_box(num_classes, input_shape, dbox, cls, anchors, strides):\n",
    "    # dbox, cls, origin_cls, anchors, strides = inputs\n",
    "    dbox = dist2bbox(dbox, anchors.unsqueeze(0), xywh=True, dim=1) * strides\n",
    "    y = torch.cat((dbox, cls.sigmoid()), 1).permute(0, 2, 1)\n",
    "    y[:, :, :4] = y[:, :, :4] / torch.Tensor([input_shape[1], input_shape[0], input_shape[1], input_shape[0]]).to(y.device)\n",
    "    return y\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image):\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = np.array(input_shape)\n",
    "    image_shape = np.array(image_shape)\n",
    "\n",
    "    if letterbox_image:\n",
    "        new_shape = np.round(image_shape * np.min(input_shape/image_shape))\n",
    "        offset = (input_shape - new_shape)/2./input_shape\n",
    "        scale = input_shape/new_shape\n",
    "\n",
    "        box_yx = (box_yx - offset) * scale\n",
    "        box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1)\n",
    "    boxes *= np.concatenate([image_shape, image_shape], axis=-1)\n",
    "    return boxes\n",
    "\n",
    "def non_max_suppression(prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4):\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for i, image_pred in enumerate(prediction):\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 4:4 + num_classes], 1, keepdim=True)\n",
    "        conf_mask = (class_conf[:, 0] >= conf_thres).squeeze()\n",
    "        image_pred = image_pred[conf_mask]\n",
    "        class_conf = class_conf[conf_mask]\n",
    "        class_pred = class_pred[conf_mask]\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        detections = torch.cat((image_pred[:, :4], class_conf.float(), class_pred.float()), 1)\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "\n",
    "        if prediction.is_cuda:\n",
    "            unique_labels = unique_labels.cuda()\n",
    "            detections = detections.cuda()\n",
    "\n",
    "        for c in unique_labels:\n",
    "            detections_class = detections[detections[:, -1] == c]\n",
    "            keep = nms(detections_class[:, :4], detections_class[:, 4], nms_thres)\n",
    "            max_detections = detections_class[keep]\n",
    "            output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections))\n",
    "        \n",
    "        if output[i] is not None:\n",
    "            output[i] = output[i].cpu().numpy()\n",
    "            box_xy, box_wh = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2]\n",
    "            output[i][:, :4] = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)\n",
    "    return output\n",
    "\n",
    "def my_callback(rgb_imgs):\n",
    "    preds = {}\n",
    "    type_mapping = {\"0\": 1, \"1\": 2, \"2\": 3, \"3\": 4, \"4\": 5, \"5\": 6, \"6\": 7}\n",
    "    type_mapping_mask = {\"0\": 0, \"1\": 8, \"2\": 9, \"3\": 10}\n",
    "    # for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "    for (img_path, img) in rgb_imgs:\n",
    "        input_image = preprocess_image(img_path)\n",
    "        image = Image.open(img_path)\n",
    "        # Assuming the model takes an input named 'input' and outputs a tensor named 'output'\n",
    "        image_shape = np.array(np.shape(image)[0:2])\n",
    "        outputs = session.run(None, {'input': input_image})\n",
    "        outputs = [torch.tensor(arr) for arr in outputs]\n",
    "        ########################################################mask\n",
    "        fea_img = torch.argmax(outputs[7].long(), 1)\n",
    "        fea_img = fea_img[0, :, :].cpu().detach().numpy()\n",
    "        fea_img = np.array(fea_img)\n",
    "        fea_img = fea_img.astype(np.uint8)\n",
    "        ########################################################mask\n",
    "        #0是对的，1是对的，6对4，5对3\n",
    "        outputs = decode_box(num_classes, input_shape, outputs[0], outputs[1], outputs[5], outputs[6])\n",
    "        results = non_max_suppression(outputs, num_classes, input_shape, \n",
    "                    image_shape, True, conf_thres = 0.5, nms_thres = 0.3)\n",
    "        pred = []\n",
    "        if results[0] is None:\n",
    "            contours, hierarchy = cv2.findContours(fea_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            pred.append({\n",
    "                \"type\": '1',\n",
    "                \"x\": 0,\n",
    "                \"y\": 0,\n",
    "                \"width\": 0,\n",
    "                \"height\": 0,\n",
    "                \"segmentation\": []\n",
    "            })\n",
    "            for contour in contours:\n",
    "                points = contour.reshape(-1, 2).tolist()\n",
    "                \n",
    "                # 将坐标点转换为字符串形式\n",
    "                points_str = [[f\"{x:.3f}\", f\"{y:.3f}\"] for x, y in points]\n",
    "                \n",
    "                # 获取轮廓像素点的值并加 7\n",
    "                type_value = int(fea_img[contour[:, 0, 1], contour[:, 0, 0]].mean()) + 7\n",
    "                \n",
    "                pred.append({\n",
    "                    \"type\": str(type_value),\n",
    "                    \"x\": -1,\n",
    "                    \"y\": -1,\n",
    "                    \"width\": -1,\n",
    "                    \"height\": -1,\n",
    "                    \"segmentation\": [points_str]\n",
    "                })\n",
    "            preds[img_path.name] = pred\n",
    "        else:\n",
    "            contours, hierarchy = cv2.findContours(fea_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            top_label   = np.array(results[0][:, 5], dtype = 'int32')\n",
    "            # top_conf    = results[0][:, 4]\n",
    "            top_boxes   = results[0][:, :4]\n",
    "            pred = []\n",
    "            for idx in range(len(top_label)):\n",
    "                pred.append({\n",
    "                    \"type\": type_mapping[str(int(top_label[idx]))],\n",
    "                    \"x\": int(top_boxes[idx, 0]),\n",
    "                    \"y\": int(top_boxes[idx, 1]),\n",
    "                    \"width\": int((top_boxes[idx, 2] - top_boxes[idx, 0])),\n",
    "                    \"height\": int((top_boxes[idx, 3] - top_boxes[idx, 1])),\n",
    "                    \"segmentation\": []\n",
    "                })\n",
    "            for contour in contours:\n",
    "                contour_points = contour.reshape(-1, 2)\n",
    "                contour_str = ', '.join(f'{x:.2f}, {y:.2f}' for x, y in contour_points)\n",
    "                contour_list = contour_str.split(',')\n",
    "                float_list = list(map(float, contour_list))\n",
    "                # 获取轮廓像素点的值并加 7\n",
    "                type_value = int(fea_img[contour[:, 0, 1], contour[:, 0, 0]].mean())\n",
    "                \n",
    "                pred.append({\n",
    "                    \"type\": type_mapping_mask[str(type_value)],\n",
    "                    \"x\": -1,\n",
    "                    \"y\": -1,\n",
    "                    \"width\": -1,\n",
    "                    \"height\": -1,\n",
    "                    \"segmentation\": [float_list]\n",
    "                })\n",
    "            preds[img_path.name] = pred \n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 starting. 1 images.\n",
      "Batch 1 done. Runtime = 0.271742582321167 seconds.\n",
      "Batch 2 starting. 1 images.\n",
      "Batch 2 done. Runtime = 0.32552552223205566 seconds.\n",
      "Batch 3 starting. 1 images.\n",
      "Batch 3 done. Runtime = 0.33602118492126465 seconds.\n",
      "Batch 4 starting. 1 images.\n",
      "Batch 4 done. Runtime = 0.390059232711792 seconds.\n",
      "Batch 5 starting. 1 images.\n",
      "Batch 5 done. Runtime = 0.32323169708251953 seconds.\n",
      "Batch 6 starting. 1 images.\n",
      "Batch 6 done. Runtime = 0.30973172187805176 seconds.\n",
      "Batch 7 starting. 1 images.\n",
      "Batch 7 done. Runtime = 0.36179661750793457 seconds.\n",
      "Batch 8 starting. 1 images.\n",
      "Batch 8 done. Runtime = 0.33627986907958984 seconds.\n",
      "Batch 9 starting. 1 images.\n",
      "Batch 9 done. Runtime = 0.32137584686279297 seconds.\n",
      "Batch 10 starting. 1 images.\n",
      "Batch 10 done. Runtime = 0.38358354568481445 seconds.\n",
      "Batch 11 starting. 1 images.\n",
      "Batch 11 done. Runtime = 0.3628561496734619 seconds.\n",
      "Batch 12 starting. 1 images.\n",
      "Batch 12 done. Runtime = 0.29770851135253906 seconds.\n",
      "Batch 13 starting. 1 images.\n",
      "Batch 13 done. Runtime = 0.30735325813293457 seconds.\n",
      "Done all batches. Total runtime = 4.327265739440918 seconds. Total energy = 0 J.\n",
      "Savings results to XML...\n",
      "XML results written successfully.\n"
     ]
    }
   ],
   "source": [
    "team.run(my_callback, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
