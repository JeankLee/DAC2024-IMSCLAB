{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Create Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../common\"))\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# import pynq\n",
    "import dac_sdc\n",
    "from IPython.display import display\n",
    "\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "\n",
    "team_name = 'T-IMI'\n",
    "dac_sdc.BATCH_SIZE = 1\n",
    "team = dac_sdc.Team(team_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the library and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some library\n",
    "# !pip install onnxruntime\n",
    "# !conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"./prune.onnx\"\n",
    "# session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "# session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "session = ort.InferenceSession(onnx_model_path, providers=['TensorrtExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Callback Function and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [1280, 1280]\n",
    "num_classes = 7\n",
    "\n",
    "def preprocess_input(image):\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def resize_image(image, size, letterbox_image):\n",
    "    iw, ih  = image.size\n",
    "    w, h    = size\n",
    "    if letterbox_image:\n",
    "        scale   = min(w/iw, h/ih)\n",
    "        nw      = int(iw*scale)\n",
    "        nh      = int(ih*scale)\n",
    "\n",
    "        image   = image.resize((nw,nh), Image.BICUBIC)\n",
    "        new_image = Image.new('RGB', size, (128,128,128))\n",
    "        new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "    else:\n",
    "        new_image = image.resize((w, h), Image.BICUBIC)\n",
    "    return new_image\n",
    "\n",
    "# Function to preprocess the image (modify as per your model's requirement)\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img  = resize_image(img, (input_shape[1], input_shape[0]), True)\n",
    "    image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(img, dtype='float32')), (2, 0, 1)), 0)\n",
    "    return image_data\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    # 左上右下\n",
    "    lt, rb  = torch.split(distance, 2, dim)\n",
    "    x1y1    = anchor_points - lt\n",
    "    x2y2    = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy    = (x1y1 + x2y2) / 2\n",
    "        wh      = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "\n",
    "def decode_box(num_classes, input_shape, dbox, cls, anchors, strides):\n",
    "    # dbox, cls, origin_cls, anchors, strides = inputs\n",
    "    dbox = dist2bbox(dbox, anchors.unsqueeze(0), xywh=True, dim=1) * strides\n",
    "    y = torch.cat((dbox, cls.sigmoid()), 1).permute(0, 2, 1)\n",
    "    y[:, :, :4] = y[:, :, :4] / torch.Tensor([input_shape[1], input_shape[0], input_shape[1], input_shape[0]]).to(y.device)\n",
    "    return y\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image):\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = np.array(input_shape)\n",
    "    image_shape = np.array(image_shape)\n",
    "\n",
    "    if letterbox_image:\n",
    "        new_shape = np.round(image_shape * np.min(input_shape/image_shape))\n",
    "        offset = (input_shape - new_shape)/2./input_shape\n",
    "        scale = input_shape/new_shape\n",
    "\n",
    "        box_yx = (box_yx - offset) * scale\n",
    "        box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1)\n",
    "    boxes *= np.concatenate([image_shape, image_shape], axis=-1)\n",
    "    return boxes\n",
    "\n",
    "def non_max_suppression(prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4):\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for i, image_pred in enumerate(prediction):\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 4:4 + num_classes], 1, keepdim=True)\n",
    "        conf_mask = (class_conf[:, 0] >= conf_thres).squeeze()\n",
    "        image_pred = image_pred[conf_mask]\n",
    "        class_conf = class_conf[conf_mask]\n",
    "        class_pred = class_pred[conf_mask]\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        detections = torch.cat((image_pred[:, :4], class_conf.float(), class_pred.float()), 1)\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "\n",
    "        if prediction.is_cuda:\n",
    "            unique_labels = unique_labels.cuda()\n",
    "            detections = detections.cuda()\n",
    "\n",
    "        for c in unique_labels:\n",
    "            detections_class = detections[detections[:, -1] == c]\n",
    "            keep = nms(detections_class[:, :4], detections_class[:, 4], nms_thres)\n",
    "            max_detections = detections_class[keep]\n",
    "            output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections))\n",
    "        \n",
    "        if output[i] is not None:\n",
    "            output[i] = output[i].cpu().numpy()\n",
    "            box_xy, box_wh = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2]\n",
    "            output[i][:, :4] = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)\n",
    "    return output\n",
    "\n",
    "def my_callback(rgb_imgs):\n",
    "    preds = {}\n",
    "    type_mapping = {\"0\": 1, \"1\": 2, \"2\": 3, \"3\": 4, \"4\": 5, \"5\": 6, \"6\": 7}\n",
    "    type_mapping_mask = {\"0\": 0, \"1\": 8, \"2\": 9, \"3\": 10}\n",
    "    # for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "    for (img_path, img) in rgb_imgs:\n",
    "        input_image = preprocess_image(img_path)\n",
    "        image = Image.open(img_path)\n",
    "        # Assuming the model takes an input named 'input' and outputs a tensor named 'output'\n",
    "        image_shape = np.array(np.shape(image)[0:2])\n",
    "        outputs = session.run(None, {'input': input_image})\n",
    "        outputs = [torch.tensor(arr) for arr in outputs]\n",
    "        ########################################################mask\n",
    "        target_width, target_height = image_shape[1], image_shape[0]\n",
    "        target_aspect = target_width / target_height\n",
    "        new_height = int(input_shape[0] / target_aspect)\n",
    "        offset = (input_shape[1] - new_height) // 2\n",
    "        fea_img = torch.argmax(outputs[7].long(), 1)\n",
    "        fea_img = fea_img[0, :, :].cpu().detach().numpy()\n",
    "        fea_img = fea_img[offset:(offset + new_height), :]\n",
    "        fea_img = cv2.resize(fea_img, (image_shape[1], image_shape[0]), interpolation = cv2.INTER_NEAREST)\n",
    "        fea_img = fea_img.astype(np.uint8)\n",
    "        fea_img = np.array(fea_img)\n",
    "        # fea_img = fea_img.astype(np.uint8)\n",
    "        \n",
    "        ########################################################mask\n",
    "        #0是对的，1是对的，6对4，5对3\n",
    "        outputs = decode_box(num_classes, input_shape, outputs[0], outputs[1], outputs[5], outputs[6])\n",
    "        results = non_max_suppression(outputs, num_classes, input_shape, \n",
    "                    image_shape, True, conf_thres = 0.5, nms_thres = 0.3)\n",
    "        pred = []\n",
    "        if results[0] is None:\n",
    "            contours, hierarchy = cv2.findContours(fea_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            pred.append({\n",
    "                \"type\": '1',\n",
    "                \"x\": 0,\n",
    "                \"y\": 0,\n",
    "                \"width\": 0,\n",
    "                \"height\": 0,\n",
    "                \"segmentation\": []\n",
    "            })\n",
    "            for contour in contours:\n",
    "                points = contour.reshape(-1, 2).tolist()\n",
    "                \n",
    "                # 将坐标点转换为字符串形式\n",
    "                points_str = [[f\"{y:.3f}\", f\"{x:.3f}\"] for x, y in points]\n",
    "                \n",
    "                # 获取轮廓像素点的值并加 7\n",
    "                type_value = int(fea_img[contour[:, 0, 1], contour[:, 0, 0]].mean()) + 6\n",
    "                \n",
    "                pred.append({\n",
    "                    \"type\": str(type_value),\n",
    "                    \"x\": -1,\n",
    "                    \"y\": -1,\n",
    "                    \"width\": -1,\n",
    "                    \"height\": -1,\n",
    "                    \"segmentation\": [points_str]\n",
    "                })\n",
    "            preds[img_path.name] = pred\n",
    "        else:\n",
    "            contours, hierarchy = cv2.findContours(fea_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            top_label   = np.array(results[0][:, 5], dtype = 'int32')\n",
    "            # top_conf    = results[0][:, 4]\n",
    "            top_boxes   = results[0][:, :4]\n",
    "            pred = []\n",
    "            for idx in range(len(top_label)):\n",
    "                pred.append({\n",
    "                    \"type\": type_mapping[str(int(top_label[idx]))],\n",
    "                    \"x\": int(top_boxes[idx, 1]),\n",
    "                    \"y\": int(top_boxes[idx, 0]),\n",
    "                    \"width\": int((top_boxes[idx, 3] - top_boxes[idx, 1])),\n",
    "                    \"height\": int((top_boxes[idx, 2] - top_boxes[idx, 0])),\n",
    "                    \"segmentation\": []\n",
    "                })\n",
    "            for contour in contours:\n",
    "                contour_points = contour.reshape(-1, 2)\n",
    "                contour_str = ', '.join(f'{x:.3f}, {y:.3f}' for x, y in contour_points)\n",
    "                contour_list = contour_str.split(',')\n",
    "                float_list = list(map(float, contour_list))\n",
    "                \n",
    "                \n",
    "                # temp = []\n",
    "                # for point in contour[2:]:  # 假设 'contour' 已经定义\n",
    "                #     x, y = int(point[0][0]), int(point[0][1])\n",
    "                #     # 确保 temp 有足够的元素来进行下面的操作\n",
    "                #     if len(temp) >= 4 and temp[-4] * temp[-3] * x * y != 0:\n",
    "                #         # 修改逻辑错误：确保在比较前将相应的项减去正确的值\n",
    "                #         if (x - temp[-4]) * (temp[-1] - temp[-3]) == (y - temp[-3]) * (temp[-2] - temp[-4]):\n",
    "                #             temp[-2] = x  # 更新最后一个x值\n",
    "                #             temp[-1] = y  # 更新最后一个y值\n",
    "                #         else:\n",
    "                #             temp.extend([x, y])\n",
    "                #     else:\n",
    "                #         temp.extend([x, y])\n",
    "                # float_list = temp        \n",
    "                        \n",
    "                # 获取轮廓像素点的值并加 7\n",
    "                type_value = int(fea_img[contour[:, 0, 1], contour[:, 0, 0]].mean())\n",
    "                \n",
    "                pred.append({\n",
    "                    \"type\": type_mapping_mask[str(type_value)],\n",
    "                    \"x\": -1,\n",
    "                    \"y\": -1,\n",
    "                    \"width\": -1,\n",
    "                    \"height\": -1,\n",
    "                    \"segmentation\": [float_list]\n",
    "                })\n",
    "            preds[img_path.name] = pred \n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 starting. 1 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 done. Runtime = 0.5496516227722168 seconds.\n",
      "Batch 2 starting. 1 images.\n",
      "Batch 2 done. Runtime = 0.5016322135925293 seconds.\n",
      "Batch 3 starting. 1 images.\n",
      "Batch 3 done. Runtime = 0.46761417388916016 seconds.\n",
      "Batch 4 starting. 1 images.\n",
      "Batch 4 done. Runtime = 0.5061137676239014 seconds.\n",
      "Batch 5 starting. 1 images.\n",
      "Batch 5 done. Runtime = 0.5274910926818848 seconds.\n",
      "Batch 6 starting. 1 images.\n",
      "Batch 6 done. Runtime = 0.530430793762207 seconds.\n",
      "Batch 7 starting. 1 images.\n",
      "Batch 7 done. Runtime = 0.4620246887207031 seconds.\n",
      "Batch 8 starting. 1 images.\n",
      "Batch 8 done. Runtime = 0.5270864963531494 seconds.\n",
      "Batch 9 starting. 1 images.\n",
      "Batch 9 done. Runtime = 0.46784424781799316 seconds.\n",
      "Batch 10 starting. 1 images.\n",
      "Batch 10 done. Runtime = 0.5206248760223389 seconds.\n",
      "Batch 11 starting. 1 images.\n",
      "Batch 11 done. Runtime = 0.47502923011779785 seconds.\n",
      "Batch 12 starting. 1 images.\n",
      "Batch 12 done. Runtime = 0.44445109367370605 seconds.\n",
      "Batch 13 starting. 1 images.\n",
      "Batch 13 done. Runtime = 0.5026335716247559 seconds.\n",
      "Batch 14 starting. 1 images.\n",
      "Batch 14 done. Runtime = 0.4359438419342041 seconds.\n",
      "Batch 15 starting. 1 images.\n",
      "Batch 15 done. Runtime = 0.4902503490447998 seconds.\n",
      "Batch 16 starting. 1 images.\n"
     ]
    }
   ],
   "source": [
    "team.run(my_callback, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
